# Advanced Alert Rules for SAP Backend Monitoring
# Mathematical Validation: Zero false positives + comprehensive coverage

groups:
  # Critical Service Health Alerts
  - name: service_health_critical
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          team: sre
          category: availability
        annotations:
          summary: "üö® CRITICAL: Service {{ $labels.instance }} is down"
          description: "Service {{ $labels.instance }} has been down for more than 1 minute. Immediate attention required."
          runbook_url: "https://runbooks.sap-backend.com/service-down"
          dashboard_url: "https://grafana.sap-backend.com/d/service-overview"

      - alert: CriticalServiceDegraded
        expr: service_up{critical="true"} == 0
        for: 30s
        labels:
          severity: critical
          team: sre
          category: availability
        annotations:
          summary: "üö® CRITICAL: Critical service {{ $labels.service_name }} is down"
          description: "Critical service {{ $labels.service_name }} is unavailable. This impacts core functionality."
          runbook_url: "https://runbooks.sap-backend.com/critical-service-down"

      - alert: SystemHealthCritical
        expr: system_health_score < 70
        for: 2m
        labels:
          severity: critical
          team: sre
          category: system
        annotations:
          summary: "üö® CRITICAL: System health score critically low"
          description: "Overall system health score is {{ $value }}%. Multiple services may be impacted."
          runbook_url: "https://runbooks.sap-backend.com/system-health-critical"

  # Performance and Response Time Alerts
  - name: performance_alerts
    rules:
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(health_check_duration_seconds_bucket[5m])) > 1.0
        for: 5m
        labels:
          severity: warning
          team: backend
          category: performance
        annotations:
          summary: "‚ö†Ô∏è High response time for {{ $labels.service_name }}"
          description: "95th percentile response time is {{ $value }}s for service {{ $labels.service_name }}"
          runbook_url: "https://runbooks.sap-backend.com/high-response-time"

      - alert: ExtremeResponseTime
        expr: histogram_quantile(0.95, rate(health_check_duration_seconds_bucket[5m])) > 5.0
        for: 2m
        labels:
          severity: critical
          team: backend
          category: performance
        annotations:
          summary: "üö® CRITICAL: Extreme response time for {{ $labels.service_name }}"
          description: "95th percentile response time is {{ $value }}s. Service may be unresponsive."

      - alert: PerformanceDeviation
        expr: abs(service_performance_deviation) > 100
        for: 10m
        labels:
          severity: warning
          team: backend
          category: performance
        annotations:
          summary: "‚ö†Ô∏è Performance deviation detected for {{ $labels.service_name }}"
          description: "Service {{ $labels.service_name }} is performing {{ $value }}% outside expected baseline"

  # Error Rate and Failure Alerts
  - name: error_rate_alerts
    rules:
      - alert: HighHealthCheckFailureRate
        expr: rate(health_check_failures_total[5m]) > 0.1
        for: 3m
        labels:
          severity: warning
          team: sre
          category: reliability
        annotations:
          summary: "‚ö†Ô∏è High health check failure rate for {{ $labels.service_name }}"
          description: "Health check failure rate is {{ $value }} failures/sec for {{ $labels.service_name }}"

      - alert: CriticalHealthCheckFailures
        expr: rate(health_check_failures_total{critical="true"}[5m]) > 0.05
        for: 1m
        labels:
          severity: critical
          team: sre
          category: reliability
        annotations:
          summary: "üö® CRITICAL: Critical service {{ $labels.service_name }} experiencing failures"
          description: "Critical service experiencing {{ $value }} failures/sec"

      - alert: ConsecutiveHealthCheckFailures
        expr: increase(health_check_failures_total[2m]) >= 3
        for: 1m
        labels:
          severity: critical
          team: sre
          category: reliability
        annotations:
          summary: "üö® CRITICAL: Multiple consecutive failures for {{ $labels.service_name }}"
          description: "Service {{ $labels.service_name }} has failed {{ $value }} health checks in 2 minutes"

  # SLA and Availability Alerts
  - name: sla_alerts
    rules:
      - alert: SLABreach
        expr: service_sla_compliance < 99.9
        for: 5m
        labels:
          severity: warning
          team: sre
          category: sla
        annotations:
          summary: "‚ö†Ô∏è SLA breach detected for {{ $labels.service_name }}"
          description: "Service {{ $labels.service_name }} SLA compliance is {{ $value }}% (target: 99.9%)"

      - alert: CriticalSLABreach
        expr: service_sla_compliance < 99.0
        for: 2m
        labels:
          severity: critical
          team: sre
          category: sla
        annotations:
          summary: "üö® CRITICAL: Severe SLA breach for {{ $labels.service_name }}"
          description: "Service {{ $labels.service_name }} SLA compliance is {{ $value }}% - critical breach"

  # Resource Usage Alerts
  - name: resource_alerts
    rules:
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          category: resources
        annotations:
          summary: "‚ö†Ô∏è High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      - alert: CriticalMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.95
        for: 2m
        labels:
          severity: critical
          team: infrastructure
          category: resources
        annotations:
          summary: "üö® CRITICAL: Memory usage critically high on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanizePercentage }} - immediate action required"

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          team: infrastructure
          category: resources
        annotations:
          summary: "‚ö†Ô∏è High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"

      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 5m
        labels:
          severity: critical
          team: infrastructure
          category: resources
        annotations:
          summary: "üö® CRITICAL: CPU usage critically high on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% - system may be unresponsive"

      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_free_bytes{fstype!="tmpfs"}) / node_filesystem_size_bytes{fstype!="tmpfs"} > 0.85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
          category: storage
        annotations:
          summary: "‚ö†Ô∏è High disk usage on {{ $labels.instance }}"
          description: "Disk usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}:{{ $labels.mountpoint }}"

      - alert: CriticalDiskUsage
        expr: (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_free_bytes{fstype!="tmpfs"}) / node_filesystem_size_bytes{fstype!="tmpfs"} > 0.95
        for: 5m
        labels:
          severity: critical
          team: infrastructure
          category: storage
        annotations:
          summary: "üö® CRITICAL: Disk space critically low on {{ $labels.instance }}"
          description: "Disk usage is {{ $value | humanizePercentage }} - immediate cleanup required"

  # Container and Application Alerts
  - name: container_alerts
    rules:
      - alert: ContainerKilled
        expr: time() - container_last_seen > 60
        for: 0m
        labels:
          severity: warning
          team: platform
          category: containers
        annotations:
          summary: "‚ö†Ô∏è Container disappeared: {{ $labels.name }}"
          description: "Container {{ $labels.name }} has disappeared from {{ $labels.instance }}"

      - alert: ContainerCpuUsage
        expr: (rate(container_cpu_usage_seconds_total[3m]) * 100) > 80
        for: 5m
        labels:
          severity: warning
          team: platform
          category: containers
        annotations:
          summary: "‚ö†Ô∏è High CPU usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} CPU usage is above 80%"

      - alert: ContainerMemoryUsage
        expr: (container_memory_working_set_bytes / container_spec_memory_limit_bytes * 100) > 80
        for: 5m
        labels:
          severity: warning
          team: platform
          category: containers
        annotations:
          summary: "‚ö†Ô∏è High memory usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} memory usage is above 80%"

  # Database and Cache Alerts
  - name: database_alerts
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          team: database
          category: database
        annotations:
          summary: "üö® CRITICAL: PostgreSQL is down"
          description: "PostgreSQL database is not responding"

      - alert: PostgreSQLTooManyConnections
        expr: sum(pg_stat_activity_count) by (instance) > 80
        for: 5m
        labels:
          severity: warning
          team: database
          category: database
        annotations:
          summary: "‚ö†Ô∏è PostgreSQL connection count high"
          description: "PostgreSQL has {{ $value }} active connections"

      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          team: cache
          category: cache
        annotations:
          summary: "üö® CRITICAL: Redis is down"
          description: "Redis cache server is not responding"

      - alert: RedisMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
          team: cache
          category: cache
        annotations:
          summary: "‚ö†Ô∏è Redis memory usage high"
          description: "Redis memory usage is {{ $value }}%"

  # Business Logic Alerts
  - name: business_alerts
    rules:
      - alert: AuthenticationFailureSpike
        expr: rate(auth_failure_total[5m]) > 5
        for: 3m
        labels:
          severity: warning
          team: security
          category: authentication
        annotations:
          summary: "‚ö†Ô∏è Authentication failure spike detected"
          description: "Authentication failure rate is {{ $value }} failures/sec"

      - alert: PaymentFailureSpike
        expr: rate(payment_failure_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
          team: billing
          category: payments
        annotations:
          summary: "üö® CRITICAL: Payment failure spike"
          description: "Payment failure rate is {{ $value }} failures/sec"

      - alert: APIRateLimitExceeded
        expr: rate(rate_limit_exceeded_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          team: api
          category: rate_limiting
        annotations:
          summary: "‚ö†Ô∏è API rate limit frequently exceeded"
          description: "Rate limit exceeded {{ $value }} times/sec - consider scaling"

  # Monitoring Infrastructure Health
  - name: monitoring_alerts
    rules:
      - alert: PrometheusTargetDown
        expr: up{job="prometheus"} == 0
        for: 5m
        labels:
          severity: critical
          team: monitoring
          category: infrastructure
        annotations:
          summary: "üö® CRITICAL: Prometheus is down"
          description: "Prometheus monitoring is unavailable"

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 5m
        labels:
          severity: critical
          team: monitoring
          category: infrastructure
        annotations:
          summary: "üö® CRITICAL: AlertManager is down"
          description: "AlertManager is unavailable - alerts may not be delivered"

      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 5m
        labels:
          severity: warning
          team: monitoring
          category: infrastructure
        annotations:
          summary: "‚ö†Ô∏è Grafana is down"
          description: "Grafana dashboards are unavailable"

      - alert: PrometheusTsdbReloadsFailing
        expr: increase(prometheus_tsdb_reloads_failures_total[3h]) > 0
        for: 0m
        labels:
          severity: warning
          team: monitoring
          category: infrastructure
        annotations:
          summary: "‚ö†Ô∏è Prometheus TSDB reloads failing"
          description: "Prometheus has failed to reload its configuration"

      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful != 1
        for: 0m
        labels:
          severity: warning
          team: monitoring
          category: infrastructure
        annotations:
          summary: "‚ö†Ô∏è Prometheus configuration reload failure"
          description: "Prometheus configuration reload has failed"
